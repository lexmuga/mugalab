---
layout: default
title: "Predictive Analytics for Text (MATH 103.1)"
description: "A modular course on text vectorization, embeddings, interpretability, and uncertainty â€” under the Model Understanding and Generative Alignment Laboratory."
---

# ğŸ“˜ Predictive Analytics for Text (MATH 103.1)
**Model Understanding and Generative Alignment Laboratory (MUGA LAB)**  
Academic Year 2025â€“2026  

---

## ğŸ§­ Overview

**Predictive Analytics for Text** is a modular course exploring supervised learning for text data,  
with a focus on **interpretability**, **reproducibility**, and **uncertainty estimation**.

Students will build pipelines for:
- Preprocessing and vectorizing textual data  
- Training classification and regression models  
- Analyzing model behavior using **SHAP** and calibration  
- Incorporating **BayesFlow** for uncertainty and probabilistic inference  

---

## ğŸ¯ Learning Objectives

By the end of this course, students will be able to:

| Objective | Description |
|------------|--------------|
| **Text Preprocessing** | Perform tokenization, lemmatization, and normalization using spaCy |
| **Vectorization** | Implement Bag-of-Words, TFâ€“IDF, and embedding-based models |
| **Supervised Learning** | Train and evaluate classification and regression pipelines |
| **Feature Analysis** | Apply SHAP, ECE, and calibration metrics for interpretability |
| **Deep Architectures** | Build MLPs, GRUs, LSTMs, and Transformers for text |
| **Uncertainty Modeling** | Integrate BayesFlow for predictive uncertainty |
| **Optimization** | Use Optuna and DEHB for hyperparameter search |
| **Communication** | Present interpretable results through scientific reporting |

---

## ğŸ—“ Course Schedule

| Week | Topics | Models / Methods |
|------|---------|------------------|
| 1 | Text preprocessing | spaCy, tokenization, lemmatization |
| 2 | Traditional vectorization | BoW, TFâ€“IDF |
| 3 | Word embeddings | Word2Vec, FastText, GloVe |
| 4 | Classification I | Perceptron, Logistic Regression, Naive Bayes |
| 5 | Classification II | SVM, Random Forest, XGBoost |
| 6 | Regression | Linear, SVR, Random Forest, XGBoost |
| 7 | Evaluation and metrics | F1, ROC AUC, MSE, MAE, RÂ² |
| 8 | Feature importance | SHAP, Optuna visualization |
| 9 | Deep models I | MLP, RNN-based |
| 10 | Deep models II | LSTM, GRU |
| 11 | Transformers and TabNet | Attention and interpretability |
| 12 | BayesFlow for Uncertainty | Bayesian inference and calibration |
| 13â€“15 | Capstone Project | Final model, report, and presentation |

---

## ğŸ§° Tools and Frameworks

| Domain | Libraries |
|--------|------------|
| **Text Processing** | `spaCy`, `NLTK`, `pandas`, `NumPy` |
| **ML & Evaluation** | `scikit-learn`, `XGBoost`, `Optuna` |
| **Deep Learning** | `PyTorch`, `transformers`, `torchtext`, `TabNet` |
| **Interpretability & Uncertainty** | `SHAP`, `ECE`, `BayesFlow`, `matplotlib` |

---

## ğŸ§® Assessment Structure

| Component | Weight |
|------------|--------|
| Chapter Critiques (6 total) | 30% |
| Long Tests (2) | 30% |
| Capstone Project | 25% |
| In-class Labs | 10% |
| Participation | 5% |

---

## ğŸ§© Capstone Project

**Goal:** Develop a complete text prediction pipeline with interpretability and uncertainty components.  

**Requirements:**
- At least one **deep architecture** (e.g., GRU, Transformer, TabNet)  
- Incorporation of **BayesFlow** or other probabilistic model  
- Comprehensive interpretability analysis (e.g., SHAP + calibration curves)  

**Deliverables:**
- ğŸ“„ Technical report (5â€“7 pages)  
- ğŸ§  Oral presentation  
- ğŸ’¾ Reproducible code and artifacts  

---

## ğŸ§  Integration with MUGA LAB Framework

This course follows the **MUGA LAB Pedagogical Stack**, linking interpretability and alignment through structured pedagogy.

| Layer | Description |
|--------|-------------|
| **Mathematical Foundation** | Optimization, calibration, uncertainty |
| **Computational Framework** | Reproducible pipelines (TFâ€“IDF, embeddings, SHAP, DEHB) |
| **Pedagogical Layer** | Structured prompt-based learning |
| **Research Integration** | Connected to *Model Understanding and Generative Alignment (2025)* |

---

## ğŸ““ Course Materials

| Module | Description | Location |
|---------|--------------|----------|
| **Traditional Vectorization** | TFâ€“IDF and Bag-of-Words workflows | [`/notebooks/traditional_vectorization.ipynb`](notebooks/traditional_vectorization.ipynb) |
| **Embedding Vectorization** | Word2Vec, FastText, GloVe, BayesFlow | [`/notebooks/embedding_vectorization.ipynb`](notebooks/embedding_vectorization.ipynb) |
| **Master Split Generation** | Reproducible train/val/test split indices | [`/scripts/generate_master_split.py`](scripts/generate_master_split.py) |
| **Helper Modules** | Preprocessing and indexing utilities | [`/helpers/`](helpers/) |

---

## ğŸ“š References

- **MUGA LAB (2025)** â€” *Model Understanding and Generative Alignment*  
  [ğŸ“„ Read Reference PDF](../../references/2025-model-understanding/model_understanding.pdf)

- Additional readings and papers will be announced throughout the course.

---

## âœ‰ï¸ Contact

ğŸ“§ **MUGA LAB** â€” mugalab.research@gmail.com  
ğŸŒ [https://lexmuga.github.io/mugalab](https://lexmuga.github.io/mugalab)
