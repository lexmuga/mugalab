{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aeca0a06",
   "metadata": {},
   "source": [
    "# MUGA LAB Calibration Analysis Notebook\n",
    "**Model Understanding and Generative Alignment Laboratory (MUGA LAB)**  \n",
    "Department of Mathematics · Ateneo de Manila University\n",
    "BS Applied Mathematics (Data Science Track)\n",
    "\n",
    "---\n",
    "\n",
    "## Objective\n",
    "\n",
    "This notebook evaluates **model calibration quality** for tabular neural networks  \n",
    "trained via the MUGA LAB tuning pipeline.\n",
    "\n",
    "We compute standard metrics (ECE, MCS, NLL), visualize reliability diagrams,  \n",
    "and log results automatically to MLflow.\n",
    "\n",
    "---\n",
    "\n",
    "## Analysis Workflow\n",
    "\n",
    "1. Load model predictions and labels.  \n",
    "2. Compute calibration metrics (ECE, MCS, NLL).  \n",
    "3. Plot reliability diagrams and confidence histograms.  \n",
    "4. Compare calibrated vs. uncalibrated models.  \n",
    "5. Export results to CSV and LaTeX summary tables.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9af9ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================================================\n",
    "# 1. Setup and Imports\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import mlflow\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from mugalab.calibration_metrics import compute_calibration_metrics\n",
    "from mugalab.reliability_diagram_utils import (\n",
    "    plot_reliability_diagram,\n",
    "    plot_confidence_histogram\n",
    ")\n",
    "\n",
    "mlflow.set_tracking_uri(\"../../results/mlruns\")\n",
    "mlflow.set_experiment(\"Calibration_Analysis\")\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use(\"seaborn-v0_8-muted\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 2. Load Model and Validation Data\n",
    "# ============================================================\n",
    "\n",
    "from mugalab.mlp_tuner_tabular_mlflow import load_model_and_data\n",
    "\n",
    "model_uri = \"runs:/<RUN_ID>/model\"   # Replace with your MLflow run ID\n",
    "model, X_val, y_val = load_model_and_data(model_uri)\n",
    "\n",
    "print(f\"Loaded model from {model_uri}\")\n",
    "print(f\"Validation data: {X_val.shape}, Labels: {y_val.shape}\")\n",
    "\n",
    "# ============================================================\n",
    "# 3. Generate Predictions and Probabilities\n",
    "# ============================================================\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    logits = model(torch.tensor(X_val, dtype=torch.float32))\n",
    "    probs = torch.softmax(logits, dim=1).numpy()\n",
    "    preds = np.argmax(probs, axis=1)\n",
    "\n",
    "print(f\"Sample probabilities:\\n{probs[:5]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31103a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================================================\n",
    "# 4. Compute Calibration Metrics\n",
    "# ============================================================\n",
    "\n",
    "metrics = compute_calibration_metrics(probs, y_val)\n",
    "\n",
    "print(\"Calibration Metrics:\")\n",
    "for k, v in metrics.items():\n",
    "    print(f\"  {k}: {v:.4f}\")\n",
    "\n",
    "results_df = pd.DataFrame([metrics])\n",
    "results_df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0fff8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 5. Visualize Reliability Diagram\n",
    "# ============================================================\n",
    "\n",
    "plot_reliability_diagram(\n",
    "    probs,\n",
    "    y_val,\n",
    "    n_bins=15,\n",
    "    title=\"Reliability Diagram (Uncalibrated Model)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1ae96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 6. Optional: Compare Calibrated vs. Uncalibrated\n",
    "# ============================================================\n",
    "\n",
    "from mugalab.undergraduate_research.experiments.temperature_scaling import TemperatureScaling\n",
    "\n",
    "calibrator = TemperatureScaling()\n",
    "calibrator.fit(torch.tensor(probs), torch.tensor(y_val))\n",
    "\n",
    "probs_calibrated = calibrator.predict(torch.tensor(probs)).numpy()\n",
    "\n",
    "# Compute metrics after calibration\n",
    "metrics_calibrated = compute_calibration_metrics(probs_calibrated, y_val)\n",
    "\n",
    "comparison = pd.DataFrame({\n",
    "    \"Metric\": [\"ECE\", \"MCS\", \"NLL\"],\n",
    "    \"Uncalibrated\": [metrics[\"ece\"], metrics[\"mcs\"], metrics[\"nll\"]],\n",
    "    \"Calibrated\": [metrics_calibrated[\"ece\"], metrics_calibrated[\"mcs\"], metrics_calibrated[\"nll\"]]\n",
    "})\n",
    "comparison[\"Improvement\"] = comparison[\"Uncalibrated\"] - comparison[\"Calibrated\"]\n",
    "comparison.round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52474bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 7. Reliability Diagram: Calibrated vs. Uncalibrated\n",
    "# ============================================================\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "plot_reliability_diagram(\n",
    "    probs,\n",
    "    y_val,\n",
    "    n_bins=15,\n",
    "    title=\"Before Calibration\",\n",
    "    ax=ax[0]\n",
    ")\n",
    "\n",
    "plot_reliability_diagram(\n",
    "    probs_calibrated,\n",
    "    y_val,\n",
    "    n_bins=15,\n",
    "    title=\"After Calibration\",\n",
    "    ax=ax[1]\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb0f15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 8. Confidence Histogram\n",
    "# ============================================================\n",
    "\n",
    "plot_confidence_histogram(\n",
    "    probs,\n",
    "    y_val,\n",
    "    title=\"Confidence Distribution (Before Calibration)\"\n",
    ")\n",
    "\n",
    "plot_confidence_histogram(\n",
    "    probs_calibrated,\n",
    "    y_val,\n",
    "    title=\"Confidence Distribution (After Calibration)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b0fa0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 9. Log Metrics to MLflow\n",
    "# ============================================================\n",
    "\n",
    "with mlflow.start_run(run_name=\"Calibration_Analysis_Run\"):\n",
    "    for key, value in metrics.items():\n",
    "        mlflow.log_metric(f\"uncalibrated_{key}\", value)\n",
    "    for key, value in metrics_calibrated.items():\n",
    "        mlflow.log_metric(f\"calibrated_{key}\", value)\n",
    "\n",
    "    mlflow.log_artifact(\"../../reports/summary/\")\n",
    "    mlflow.set_tag(\"analysis\", \"calibration_comparison\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4619fbf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 10. Export Results for Reporting\n",
    "# ============================================================\n",
    "\n",
    "output_path = \"../../reports/summary/calibration_comparison.csv\"\n",
    "comparison.to_csv(output_path, index=False)\n",
    "print(f\"Calibration comparison exported to: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4401e8",
   "metadata": {},
   "source": [
    "## Discussion and Interpretation\n",
    "\n",
    "In the **Results and Discussion** section of the thesis, include:\n",
    "\n",
    "- Quantitative changes in calibration metrics (ECE, MCS, NLL).  \n",
    "- Visual comparison between reliability diagrams.  \n",
    "- Interpretation of whether temperature scaling improved model confidence alignment.  \n",
    "- Discussion of residual miscalibration and potential future improvements.\n",
    "\n",
    "**Example phrasing:**\n",
    "\n",
    "> “Temperature scaling reduced the Expected Calibration Error from 0.024 to 0.011,  \n",
    "> indicating that post-hoc calibration substantially improved model reliability  \n",
    "> without affecting predictive accuracy.”"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0aee7d8",
   "metadata": {},
   "source": [
    "## Notebook Summary\n",
    "\n",
    "| Section | Description |\n",
    "|---------|-------------|\n",
    "| 1–3 | Setup, model load, and predictions |\n",
    "| 4 | Metric computation |\n",
    "| 5–7 | Visualization and comparison |\n",
    "| 8 | Confidence distribution |\n",
    "| 9–10 | Logging and export |\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "math103mps_env2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
