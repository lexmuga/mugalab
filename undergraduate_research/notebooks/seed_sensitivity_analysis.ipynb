{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93755e78",
   "metadata": {},
   "source": [
    "# MUGA LAB Seed Sensitivity and Reproducibility Analysis\n",
    "\n",
    "**Model Understanding and Generative Alignment Laboratory (MUGA LAB)**  \n",
    "Department of Mathematics, Ateneo de Manila University \n",
    "BS Applied Mathematics (Data Science Track)\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## Objective\n",
    "\n",
    "This notebook evaluates **model performance stability across random seeds**  \n",
    "to quantify the **reproducibility** of calibration and distillation experiments.\n",
    "\n",
    "It integrates with:\n",
    "- `seed_sensitivity_utils.py` (multi-seed evaluation)\n",
    "- `mlp_tuner_tabular_mlflow.py` (Optuna/DEHB tuning)\n",
    "- `calibration_metrics.py` (ECE, MCS, NLL computation)\n",
    "- `reliability_diagram_utils.py` (calibration visualization)\n",
    "\n",
    "---\n",
    "\n",
    "## nalysis Workflow\n",
    "\n",
    "1. Import dependencies and connect to MLflow.\n",
    "2. Load the best model or configuration from tuning results.\n",
    "3. Run multi-seed evaluation using `evaluate_across_seeds()` or `evaluate_metrics_across_seeds()`.\n",
    "4. Compute and visualize:\n",
    "   - Mean ± standard deviation of metrics across seeds.\n",
    "   - Calibration plots for best and worst seeds.\n",
    "   - Stability ranking of configurations.\n",
    "5. Export results as CSV and summary report.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83ad71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mugalab/undergraduate_research/notebooks/seed_sensitivity_analysis.ipynb\n",
    "\n",
    "# ============================================================\n",
    "# 1. Setup\n",
    "# ============================================================\n",
    "\n",
    "import mlflow\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from mugalab.undergraduate_research.utils.seed_sensitivity_utils import (\n",
    "    evaluate_metrics_across_seeds,\n",
    "    export_seed_results\n",
    ")\n",
    "from mugalab.calibration_metrics import compute_calibration_metrics\n",
    "from mugalab.reliability_diagram_utils import plot_reliability_diagram\n",
    "\n",
    "# Optional: Use inline plotting\n",
    "%matplotlib inline\n",
    "\n",
    "mlflow.set_tracking_uri(\"../../results/mlruns\")\n",
    "mlflow.set_experiment(\"Seed_Sensitivity_Analysis\")\n",
    "\n",
    "# Define seed list and experiment context\n",
    "seeds = [0, 21, 42, 84, 126]\n",
    "task = \"classification\"\n",
    "\n",
    "# ============================================================\n",
    "# 2. Load Trained Model and Data\n",
    "# ============================================================\n",
    "\n",
    "import torch\n",
    "from mugalab.mlp_tuner_tabular_mlflow import load_model_and_data\n",
    "\n",
    "# Example function to load dataset and model checkpoint\n",
    "# Adjust paths for your project\n",
    "model_uri = \"runs:/<RUN_ID>/model\"\n",
    "model, X_val, y_val = load_model_and_data(model_uri)\n",
    "\n",
    "print(f\"Loaded model from {model_uri}\")\n",
    "print(f\"Validation data: {X_val.shape}, Labels: {y_val.shape}\")\n",
    "\n",
    "# ============================================================\n",
    "# 3. Define Evaluation Function\n",
    "# ============================================================\n",
    "\n",
    "def evaluate_calibration(model, X_val, y_val, task=\"classification\"):\n",
    "    \"\"\"\n",
    "    Evaluate calibration metrics for a given model and dataset.\n",
    "    Returns a dictionary of metrics.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(torch.tensor(X_val, dtype=torch.float32))\n",
    "        probs = torch.softmax(logits, dim=1).numpy()\n",
    "        preds = np.argmax(probs, axis=1)\n",
    "\n",
    "    metrics = compute_calibration_metrics(probs, y_val)\n",
    "    return {\n",
    "        \"ECE\": metrics[\"ece\"],\n",
    "        \"MCS\": metrics[\"mcs\"],\n",
    "        \"NLL\": metrics[\"nll\"],\n",
    "        \"Accuracy\": np.mean(preds == y_val)\n",
    "    }\n",
    "\n",
    "# ============================================================\n",
    "# 4. Run Multi-Seed Evaluation\n",
    "# ============================================================\n",
    "\n",
    "from mugalab.undergraduate_research.utils.seed_sensitivity_utils import evaluate_metrics_across_seeds\n",
    "\n",
    "results_df = evaluate_metrics_across_seeds(\n",
    "    func=evaluate_calibration,\n",
    "    seeds=seeds,\n",
    "    metric_keys=[\"ECE\", \"MCS\", \"NLL\", \"Accuracy\"],\n",
    "    mlflow_experiment=\"Seed_Sensitivity_Analysis\",\n",
    "    model=model,\n",
    "    X_val=X_val,\n",
    "    y_val=y_val,\n",
    "    task=task\n",
    ")\n",
    "\n",
    "results_df.head()\n",
    "\n",
    "# ============================================================\n",
    "# 5. Compute Summary Statistics\n",
    "# ============================================================\n",
    "\n",
    "summary = results_df.describe()[[\"ECE\", \"MCS\", \"NLL\", \"Accuracy\"]].T\n",
    "summary[\"std / mean\"] = summary[\"std\"] / summary[\"mean\"]\n",
    "summary = summary.round(4)\n",
    "summary\n",
    "\n",
    "# ============================================================\n",
    "# 6. Visualization: Metric Variance Across Seeds\n",
    "# ============================================================\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "for metric in [\"ECE\", \"MCS\", \"NLL\", \"Accuracy\"]:\n",
    "    plt.plot(results_df[\"seed\"], results_df[metric], marker=\"o\", label=metric)\n",
    "plt.title(\"Metric Variability Across Seeds\")\n",
    "plt.xlabel(\"Random Seed\")\n",
    "plt.ylabel(\"Metric Value\")\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# ============================================================\n",
    "# 7. Reliability Diagrams for Extremes\n",
    "# ============================================================\n",
    "\n",
    "best_seed = results_df.loc[results_df[\"ECE\"].idxmin(), \"seed\"]\n",
    "worst_seed = results_df.loc[results_df[\"ECE\"].idxmax(), \"seed\"]\n",
    "\n",
    "print(f\"Best calibration (lowest ECE): Seed {best_seed}\")\n",
    "print(f\"Worst calibration (highest ECE): Seed {worst_seed}\")\n",
    "\n",
    "# Assuming model can be reloaded per seed if trained separately\n",
    "for seed in [best_seed, worst_seed]:\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    logits = model(torch.tensor(X_val, dtype=torch.float32))\n",
    "    probs = torch.softmax(logits, dim=1).numpy()\n",
    "    plot_reliability_diagram(probs, y_val, title=f\"Reliability Diagram (Seed {seed})\")\n",
    "\n",
    "# ============================================================\n",
    "# 8. Export Results\n",
    "# ============================================================\n",
    "\n",
    "output_path = \"../../reports/summary/seed_sensitivity_results.csv\"\n",
    "export_seed_results(results_df, output_path)\n",
    "print(f\"Results exported to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c489c7",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## Discussion and Reporting\n",
    "\n",
    "In the **Results and Discussion** section of the thesis, report:\n",
    "\n",
    "- Mean ± std for all calibration metrics.\n",
    "- Identify whether performance variance is statistically significant.\n",
    "- Visualize reliability diagrams for best/worst seeds.\n",
    "- Discuss stability implications for reproducibility.\n",
    "\n",
    "**Example phrasing:**\n",
    "> “Across five random seeds, Expected Calibration Error (ECE) varied between 0.014 and 0.029,  \n",
    "> indicating moderate sensitivity to initialization. The variance-to-mean ratio suggests that  \n",
    "> calibration remains consistent across different random initializations.”\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## Notebook Summary\n",
    "\n",
    "| Section | Purpose |\n",
    "|---------|----------|\n",
    "| 1–2.  Setup & Load  | Connect to MLflow and load model/data. |\n",
    "| 3. Evaluation Function | Compute ECE, MCS, NLL, and accuracy. |\n",
    "| 4–5. Multi-seed Analysis | Aggregate results across seeds. |\n",
    "| 6–7. Visualization | Plot metric variance and reliability diagrams. |\n",
    "| 8. Export | Save CSV to `reports/summary/`. |\n",
    "| Discussion | Interpret results for thesis inclusion. |\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "math103mps_env2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
